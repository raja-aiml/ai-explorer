# =========================================================================
# CONFIGURATION: LLM Evaluation Framework
# =========================================================================

project:
  name: networking-expert-eval
  description: Evaluate domain-specific LLMs for networking expertise
  version: "1.0.0"

logging:
  level: DEBUG
  file: logs/app.log

output:
  formats:
    - markdown
    - json
    - yaml
  report_dir: reports/

data:
  synthetic_generator: kiln
  dataset_path: data/test_dataset.yaml
  output_path: outputs/generated_data.yaml

cli:
  commands:
    - name: generate-data
      description: Generate synthetic data using Kiln
    - name: evaluate-model
      description: Evaluate model using defined metrics
    - name: generate-report
      description: Generate structured evaluation reports

metrics:
  - name: BLEU
    description: Measures n-gram precision overlap
    level: lexical
    use_case: machine translation, QA
    library: [sacrebleu, nltk]

  - name: ROUGE
    description: Measures overlapping sequence recall
    level: lexical
    use_case: summarization, QA
    library: [rouge-score, evaluate]

  - name: BERTScore
    description: Computes semantic similarity
    level: semantic
    use_case: text generation, QA
    library: [bert-score]

  - name: TruthfulQA
    description: Detects factual correctness
    level: factual
    use_case: hallucination detection
    library: [truthfulqa, evaluate]

  - name: Perplexity
    description: Evaluates fluency of generated text
    level: fluency
    use_case: language modeling
    library: [transformers, evaluate]

  - name: Exact Match
    description: Checks exact string match with reference
    level: binary
    use_case: closed-form QA
    library: [evaluate]

  - name: F1 Score
    description: Harmonic mean of precision and recall
    level: token-level
    use_case: NER, QA
    library: [sklearn, evaluate]

  - name: MAUVE
    description: Measures divergence between distributions
    level: generative
    use_case: open-ended generation
    library: [mauve-text]

  - name: METEOR
    description: Alignment for generation using semantics
    level: lexical
    use_case: translation, QA
    library: [nltk]

observability:
  enabled: true
  exporter: otlp
  endpoint: http://localhost:4317
  service_name: llm-eval-cli
  log_spans: true