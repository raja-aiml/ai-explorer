# AI Explorer

> Prompt generation + LLM interaction CLI tool powered by Go, Cobra, and Pongo2.

---

## âœ¨ Features

- ğŸ§  **Prompt Generator**: Generate prompts using YAML config + template.
- ğŸ¤– **LLM CLI**: Send raw prompts to LLMs (e.g., OpenAI, Ollama).
- ğŸ› ï¸ **Template System**: Uses `pongo2` (Jinja-like) templating.
- ğŸ“ **Category-based routing**: Organize prompts under `resources/<category>/<topic>`.
- âœ… **100% Unit + E2E Tests**: Ginkgo-powered full coverage.
- ğŸ“œ **Shell Completion**: Autogenerate completions for bash, zsh, fish, PowerShell.

---

## ğŸš€ Quickstart

```bash
# Build
task build

# Generate a prompt (auto path resolution)
ai-explorer prompt --category=topics --topic=git

# Preview prompt
ai-explorer prompt --category=topics --topic=git --preview

# Send prompt to LLM
ai-explorer llm --provider=ollama --model=phi4 --prompt=resources/topics/git/prompt.txt

# Generate zsh completion script
task completion


-------

To Do 

Thanks for the detailed dumpâ€”your project is looking excellent! Itâ€™s already modular, cleanly structured, and tightly tested with both unit and E2E coverage.

Since there wasnâ€™t a direct question in your last message, hereâ€™s a checklist of refinement and polish suggestions as youâ€™re prepping for a stable release:

â¸»

âœ… 1. CLI Experience (UX) Polish
	â€¢	âœ… You already have completion for bash, zsh, fish, and powershell.
	â€¢	ğŸ“Œ Add --help examples for the llm command similar to how prompt uses resources/help/examples.md.
	â€¢	ğŸ“Œ Add descriptions or defaults to printed CLI flag values via cmd/llm/llm.go (using cmd.Flags().VisitAll() + SetUsageTemplate()).

â¸»

âœ… 2. Project Cleanup / Consistency
	â€¢	ğŸ” You have a resources/paths.go and a separate paths/manager.go. Consider merging these into a unified paths package to avoid ambiguity.
	â€¢	ğŸ’¡ Suggest: move PathResolver into paths (renaming resources/paths.go to paths/resolver.go) to align directory and purpose.

â¸»

âœ… 3. Observability or Debug Mode
	â€¢	ğŸ’¬ Add a global --verbose or --debug flag to:
	â€¢	Print intermediate paths used (template, config, output)
	â€¢	Print which model/provider is being used
	â€¢	ğŸ“ Suggest using your logger interface for thisâ€”replace log.Fatalf in LLMRunner with logger.Fatalf to decouple from hardcoded logging.

â¸»

âœ… 4. Output Paths for llm and prompt
	â€¢	ğŸ§¼ Paths like resources/classification/router/prompt.txt and llm.txt are useful but maybe too hardcoded.
	â€¢	ğŸ“Œ Let the CLI auto-pick a timestamped name like:

output = fmt.Sprintf("resources/classification/router/%s.llm.txt", time.Now().Format("20060102-150405"))

â€¦if the --output flag is not provided.

â¸»

âœ… 5. Release Tasks

Consider adding the following to your Taskfile.yaml:

release:
  desc: "Build binary and tag a release"
  cmds:
    - task: build
    - git tag v1.0.0
    - git push origin v1.0.0

And for completions on install:

eval "$(.build/ai-explorer completion zsh)"



â¸»

âœ… 6. Possible Nice-to-Have
	â€¢	ğŸ§  Add a --dry-run to llm that only prints what prompt would be sent (useful for debugging prompt logic).
	â€¢	ğŸ“¦ Add a config.yaml flag to llm to allow specifying all config from YAML (reusing llm/config.ConfigLoader).

â¸»

Let me know if you want a code patch for anything above (like swapping log.Fatalf in LLMRunner for your logger, or merging the paths packages cleanly). Also happy to help build a real v1.0.0 changelog and release checklist.